{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; font-size:24px; font-weight: bold\"> FIT1043 Introduction to Data Science </div>\n",
    "<div style=\"text-align: right; font-size:24px; font-weight: bold\"> Assignment 2 </div>\n",
    "<div style=\"text-align: right; font-size:18px; margin-top: 10px\"> Michelle Adeline </div>\n",
    "<div style=\"text-align: right; font-size:18px\"> 31989101 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this report is to read and preprocess data derived from a set of essays, then conduct predictive analysis on this data using machine learning, and finally to evaluate the accuracy of the predictions. The dataset consists of numeric information on a set of student-written essays and with this information the goal is to create an appropriate machine learning model to perform automated essay scoring.\n",
    "\n",
    "The report has the following outline:\n",
    "1. Introduction\n",
    "2. Downloading and Importing Libraries\n",
    "3. Reading and Describing the Data\n",
    "4. Supervised Learning\n",
    "5. Feature Selection\n",
    "6. Splitting Data into Train/Test Set\n",
    "7. Classification\n",
    "8. Normalization/Standardization\n",
    "8. SVM\n",
    "10. Model Evaluation\n",
    "11. Kaggle Submission\n",
    "12. Conclusion\n",
    "13. References\n",
    "\n",
    "Note that in part 9 - Kaggle Submission, a Random Forest Classifier is used instead of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Downloading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to download the libraries that are used in this report. \n",
    "\n",
    "Here, there is only one library to download and it is *imbalanced-learn* which is an open-source library that provides tools to aid in classification when there are imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# might need to restart kernel and rerun this cell after installation\n",
    "! pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the required libraries, we need to import the necessary libraries and modules:\n",
    "1. **pandas**, which is an open source data analysis tool for python. It allows the usage of convenient data structures such as ***DataFrame*** along with a collection of functions that will greatly aid in reading files and manipulating DataFrames. We can also use the *as* keyword to specify a simpler, and shorter name - *pd* to refer to the library from this point onwards.\n",
    "2. **numpy**, which is an open source python library that is very useful when working with arrays and matrices. It also provides a large collection of mathematical functions to operate on these arrays. Here we use the *as* keyword to specify a simpler, and shorter name - *np* to refer to the library from this point onwards.\n",
    "3. **sklearn**, also known as scikit-learn is an open-source machine learning library for python. It provides easy-to-use tools for predictive data analysis and features a vast range of classification, regression, and clustering algorithms. Here we are importing the various modules in the library instead of just importing the library for ease of use.\n",
    "4. **imblearn**, also known as imbalanced-learn, is an open-source library that can improve accuracy of classification algorithms when there are imbalanced classes\n",
    "5. **collections** is a built-in python module which implements specialized container datatypes but here we are only using the Counter class from this module that is used for counting hashable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, cohen_kappa_score, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Describing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Reading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the *'FIT1043-Essay-Features.csv'* file with the *.read_csv* function from pandas and assign the resulting DataFrame to the *essay_data* variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_data = pd.read_csv('FIT1043-Essay-Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;Ensuring the data was read correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the file has been read properly, we can first check whether the resulting DataFrame has the same number of rows and columns as the original csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that there are 1332 rows and 19 columns. Comparing this with the original csv file, which also has 1332 rows (excluding the first row which is used as a header and is thus not counted as a row in the DataFrame) and 19 columns, we can see that the resulting DataFrame has the same number of rows and columns as the original file.\n",
    "\n",
    "Next we can display the first and last 5 rows of the *essay_data* DataFrame to check whether the contents of the cells have been read properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253</td>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0     1457   2153    426      14            6             0         5.053991   \n",
       "1      503   1480    292       9            7             0         5.068493   \n",
       "2      253   3964    849      19           26             1         4.669022   \n",
       "3      107    988    210       8            7             0         4.704762   \n",
       "4     1450   3139    600      13            8             0         5.231667   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         16          0          26.625000  423.995272         0.995294   \n",
       "1         11          0          26.545455  290.993103         0.996552   \n",
       "2         49          2          17.326531  843.990544         0.994100   \n",
       "3         12          0          17.500000  207.653784         0.988828   \n",
       "4         24          1          25.000000  594.652150         0.991087   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           207                  0.485915            105   \n",
       "1           148                  0.506849             77   \n",
       "2           285                  0.335689            130   \n",
       "3           112                  0.533333             62   \n",
       "4           255                  0.425000            165   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                   0.246479        424      412      4  \n",
       "1                   0.263699        356      345      4  \n",
       "2                   0.153121        750      750      4  \n",
       "3                   0.295238        217      209      3  \n",
       "4                   0.275000        702      677      4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1151</td>\n",
       "      <td>2404</td>\n",
       "      <td>467</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5.147752</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>21.227273</td>\n",
       "      <td>462.987069</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>200</td>\n",
       "      <td>0.428266</td>\n",
       "      <td>113</td>\n",
       "      <td>0.241970</td>\n",
       "      <td>529</td>\n",
       "      <td>519</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>1015</td>\n",
       "      <td>1182</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.904564</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>15.062500</td>\n",
       "      <td>238.655462</td>\n",
       "      <td>0.990272</td>\n",
       "      <td>94</td>\n",
       "      <td>0.390041</td>\n",
       "      <td>67</td>\n",
       "      <td>0.278008</td>\n",
       "      <td>293</td>\n",
       "      <td>283</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1345</td>\n",
       "      <td>1814</td>\n",
       "      <td>363</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.997245</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>27.923077</td>\n",
       "      <td>362.329640</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>170</td>\n",
       "      <td>0.468320</td>\n",
       "      <td>107</td>\n",
       "      <td>0.294766</td>\n",
       "      <td>427</td>\n",
       "      <td>415</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>344</td>\n",
       "      <td>1427</td>\n",
       "      <td>287</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.972125</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>22.076923</td>\n",
       "      <td>284.657277</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>144</td>\n",
       "      <td>0.501742</td>\n",
       "      <td>83</td>\n",
       "      <td>0.289199</td>\n",
       "      <td>323</td>\n",
       "      <td>312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>1077</td>\n",
       "      <td>2806</td>\n",
       "      <td>542</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.177122</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>24.636364</td>\n",
       "      <td>538.988889</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>284</td>\n",
       "      <td>0.523985</td>\n",
       "      <td>155</td>\n",
       "      <td>0.285978</td>\n",
       "      <td>596</td>\n",
       "      <td>575</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "1327     1151   2404    467      16           10             0   \n",
       "1328     1015   1182    241       0           14             0   \n",
       "1329     1345   1814    363       5           11             0   \n",
       "1330      344   1427    287       5            8             0   \n",
       "1331     1077   2806    542      24            6             0   \n",
       "\n",
       "      avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "1327         5.147752         22          0          21.227273  462.987069   \n",
       "1328         4.904564         16          0          15.062500  238.655462   \n",
       "1329         4.997245         13          3          27.923077  362.329640   \n",
       "1330         4.972125         13          1          22.076923  284.657277   \n",
       "1331         5.177122         22          3          24.636364  538.988889   \n",
       "\n",
       "      POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "1327         0.991407           200                  0.428266            113   \n",
       "1328         0.990272            94                  0.390041             67   \n",
       "1329         0.998153           170                  0.468320            107   \n",
       "1330         0.991837           144                  0.501742             83   \n",
       "1331         0.994444           284                  0.523985            155   \n",
       "\n",
       "      synonym_words/total_words  unstemmed  stemmed  score  \n",
       "1327                   0.241970        529      519      4  \n",
       "1328                   0.278008        293      283      3  \n",
       "1329                   0.294766        427      415      3  \n",
       "1330                   0.289199        323      312      3  \n",
       "1331                   0.285978        596      575      4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can be certain that the data has been read properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Describing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the column values are numeric, however, note that the last column (*score*) is numeric, discrete, *and* categorical. This is because it can only take one of 6 discrete values (1, 2, 3, 4, 5, or 6).\n",
    "\n",
    "***Central tendency:***\n",
    "- **essayid**\n",
    "    - *Note that this column contains the unique ids of the essays and has no bearing on the content of the essay itself, thus these statistical descriptions might not be particularly meaningful or useful*\n",
    "    - The average essay id is 905\n",
    "    - The median essay id is 915\n",
    "- **chars**\n",
    "    - The average number of characters in an essay is 2102\n",
    "    - The median number of characters in an essay is 2030\n",
    "- **words**\n",
    "    - The average number of words in an essay is 424\n",
    "    - The median number of words in an essay is 411\n",
    "- **commas**\n",
    "    - The average number of commas in an essay is 15\n",
    "    - The median number of commas in an essay is 13\n",
    "- **apostrophes**\n",
    "    - The average number of apostrophes in an essay is 8.1\n",
    "    - The median number of apostrophes in an essay is 6.0\n",
    "- **punctuations**\n",
    "    - *Note that punctuations here excludes commas, apostrophes, period, question marks*\n",
    "    - The average number of punctuations in an essay is 0.5\n",
    "    - The median number of punctuations in an essay is 0.0\n",
    "- **avg_word_length**\n",
    "    - The average number of characters in a word in an essay is 4.9\n",
    "    - The median number of characters in a word in an essay is also 4.9\n",
    "- **sentences**\n",
    "    - The average number of sentences in an essay is 20\n",
    "    - The median number of sentences in an essay is 18\n",
    "- **questions**\n",
    "    - The average number of questions in an essay is 1.2\n",
    "    - The median number of questions in an essay is 1.0\n",
    "- **avg_word_sentence**\n",
    "    - The average number of words per sentence in an essay is 24\n",
    "    - The median number of words per sentence in an essay is 22\n",
    "- **POS**\n",
    "    - *Total number of Part-of-Speech discovered in the essay. This means the number of words that have similar grammatical properties*\n",
    "    - The average number of Part-of-Speech discovered in an essay is 421\n",
    "    - The median number of Part-of-Speech discovered in an essay is 407\n",
    "- **POS/total_words**\n",
    "    - The average ratio of Part-of-Speech discovered to the total number of words in an essay is 1.0\n",
    "    - The median ratio of Part-of-Speech discovered to the total number of words in an essay is also 1.0\n",
    "- **prompt_words**\n",
    "    - The average number of words related to the prompt in an essay is 199\n",
    "    - The median number of words related to the prompt in an essay is 193\n",
    "- **prompt_words/total_words**\n",
    "    - The average ratio of the number of words related to the prompt to the total number of words in an essay is 0.5\n",
    "    - The median ratio of the number of words related to the prompt to the total number of words in an essay is also 0.5\n",
    "- **synonym_words**\n",
    "    - The average number of synonymous words in an essay is 110\n",
    "    - The median number of synonymous words in an essay is 108\n",
    "- **synonym_words/total_words**\n",
    "    - The average ratio of the number of synonymous words to the total number of words in an essay is 0.3\n",
    "    - The median ratio of the number of synonymous words to the total number of words in an essay is also 0.3\n",
    "- **unstemmed**\n",
    "    - *Unstemmed words refers to words that cannot be further reduced to their base forms*\n",
    "    - The average number of unstemmed words in an essay is 469\n",
    "    - The median number of unstemmed words in an essay is 463\n",
    "- **stemmed**\n",
    "    - *Stemmed words refers to words that **can** be further reduced to their base forms*\n",
    "    - The average number of stemmed words in an essay is 456\n",
    "    - The median number of stemmed words in an essay is 448\n",
    "- **score**\n",
    "    - The average rating of an essay is 3.4\n",
    "    - The median rating of an essay is 3.0\n",
    "    \n",
    "***Variability***:\n",
    "\n",
    "*Range can be calculated by subtracting the smallest value in a column from the largest value in a column (max - min)*\n",
    "- **essayid**\n",
    "    - There is a large range of 1799 between the maximum and minimum essay ids\n",
    "    - The variance in the essay id between essays is 277400\n",
    "- **chars**\n",
    "    - There is a large range of 5973 between the essay with the most characters and the essay with the least number of characters\n",
    "    - The variance in the number of characters between essay is 749893\n",
    "- **words**\n",
    "    - There is a large range of 1134 between the essay with the most words and the essay with the least number of words\n",
    "    - The variance in the number of words between essays is 29541\n",
    "- **commas**\n",
    "    - There is a very large range of 72 between the essay with the most commas and the essay with the least number of commas\n",
    "    - The variance in the number of commas between essays is 119\n",
    "- **apostrophes**\n",
    "    - There is a large range of 49 between the essay with the most apostrophes and the essay with the least number of apostrophes\n",
    "    - The variance in the number of apostrophes between essays is 37.5\n",
    "- **punctuations**\n",
    "    - There is a very large range of 26 (seeing as the mean is only 0.5) between the essay with the most punctuations and the essay with the least number of punctuations\n",
    "    - The variance in the number of punctuations between essays is 1.6\n",
    "- **avg_word_length**\n",
    "    - There is a relatively small range of 3.5 between the essay with the highest average word length and the essay with the lowest average word length\n",
    "    - The variance in the average word length between essays is 0.053\n",
    "- **sentences**\n",
    "    - There is a very large range of 642 between the essay with the most number of sentences and the essay with the least number of sentences\n",
    "    - The variance in the number of sentences between essays is 369\n",
    "- **questions**\n",
    "    - There is a large range of 17 (seeing as the mean is 1.2) between the essay with the most number of questions and the essay with the least number of questions\n",
    "    - The variance in the number of questions between essays is 3.4\n",
    "- **avg_word_sentence**\n",
    "    - There is a very large range of 302 between the essay with the highest average number of words per sentence and the essay with the lowest average number of words per sentence\n",
    "    - The variance in the number of average words per sentence between essays is 124.5\n",
    "- **POS**\n",
    "    - There is a very large range of 1123 between the essay with the most number of POS discovered and the essay with the least number of POS discovered\n",
    "    - The variance in the number of Part-Of-Speech between essays is 29236\n",
    "- **POS/total_words**\n",
    "    - There is a very small range of 0.08 between the essay with the highest number of Part-Of-Speech discovered per total words and the essay with the lowest number of Part-Of-Speech discovered per total words\n",
    "    - The variance in the number of Part-Of-Speech per total number of words between essays is 5.3e-5\n",
    "- **prompt_words**\n",
    "    - There is a large range of 655 between the essay with the most number of prompt words and the essay with the least number of prompt words\n",
    "    - The variance in the number of words related to the prompt between essays is 6844\n",
    "- **prompt_words/total_words**\n",
    "    - There is a range of 0.67 between the essay with the highest proportion of prompt words to total words and the essay with the lowest proportion of prompt words to total words\n",
    "    - The variance in the ratio of words related to the prompt to the total number of words between essays is 2.8e-3\n",
    "- **synonym_words**\n",
    "    - There is a range of 344 between the essay with the highest number of synonymous words and the essay with the lowest number of synonymous words\n",
    "    - The variance in the number of synonymous words between essays is 1933\n",
    "- **synonym_words/total_words**\n",
    "    - There is a relatively large range of 0.4 between the essay with the highest proportion of synonymous words to total words and the essay with the lowest proportion of synonymous words to total words\n",
    "    - The variance in the ratio of synonymous words to the total number of words between essays is 1.5e-3\n",
    "- **unstemmed**\n",
    "    - There is a large range of 702 between the essay with the highest number of unstemmed words and the essay with the lowest number of unstemmed words\n",
    "    - The variance in the number of unstemmed words (words that were not reduced further to their base form) between essays is 25423\n",
    "- **stemmed**\n",
    "    - There is a large range of 700 between the maximum and minimum number of stemmed words (words that have been reduced further to their base form) in an essay\n",
    "    - The variance in the number of stemmed words (words that have been reduced further to their base form) between essays is 24258\n",
    "- **score**\n",
    "    - There is a range of 5 between the maximum and minimum score of an essay, which makes sense since the possible scores are 1, 2, 3, 4, 5, or 6\n",
    "    - The variance in the score between essays is 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>905.27027</td>\n",
       "      <td>2101.745495</td>\n",
       "      <td>424.485736</td>\n",
       "      <td>14.667417</td>\n",
       "      <td>8.141141</td>\n",
       "      <td>0.47973</td>\n",
       "      <td>4.939762</td>\n",
       "      <td>19.704204</td>\n",
       "      <td>1.222973</td>\n",
       "      <td>23.884687</td>\n",
       "      <td>420.596542</td>\n",
       "      <td>0.989935</td>\n",
       "      <td>198.913664</td>\n",
       "      <td>0.469164</td>\n",
       "      <td>110.16967</td>\n",
       "      <td>0.263846</td>\n",
       "      <td>468.987988</td>\n",
       "      <td>455.507508</td>\n",
       "      <td>3.427177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>526.68760</td>\n",
       "      <td>865.963750</td>\n",
       "      <td>171.873730</td>\n",
       "      <td>10.920781</td>\n",
       "      <td>6.124520</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>0.231071</td>\n",
       "      <td>19.202731</td>\n",
       "      <td>1.847446</td>\n",
       "      <td>11.160020</td>\n",
       "      <td>170.985111</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>82.729266</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>43.96192</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>159.447449</td>\n",
       "      <td>155.751220</td>\n",
       "      <td>0.774275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.231322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084112</td>\n",
       "      <td>35.647059</td>\n",
       "      <td>0.924771</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>442.75000</td>\n",
       "      <td>1527.250000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.791679</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>305.406284</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.435709</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>350.750000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>914.50000</td>\n",
       "      <td>2029.500000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.946059</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.030331</td>\n",
       "      <td>406.982869</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.465852</td>\n",
       "      <td>107.50000</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1369.75000</td>\n",
       "      <td>2613.500000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.092938</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.048234</td>\n",
       "      <td>520.739458</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.288277</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>561.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1799.00000</td>\n",
       "      <td>6142.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>5.681429</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>1158.984563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>669.000000</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>355.00000</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          essayid        chars        words       commas  apostrophes  \\\n",
       "count  1332.00000  1332.000000  1332.000000  1332.000000  1332.000000   \n",
       "mean    905.27027  2101.745495   424.485736    14.667417     8.141141   \n",
       "std     526.68760   865.963750   171.873730    10.920781     6.124520   \n",
       "min       0.00000   169.000000    36.000000     0.000000     2.000000   \n",
       "25%     442.75000  1527.250000   310.000000     7.000000     4.000000   \n",
       "50%     914.50000  2029.500000   411.000000    13.000000     6.000000   \n",
       "75%    1369.75000  2613.500000   525.000000    21.000000    11.000000   \n",
       "max    1799.00000  6142.000000  1170.000000    72.000000    51.000000   \n",
       "\n",
       "       punctuations  avg_word_length    sentences    questions  \\\n",
       "count    1332.00000      1332.000000  1332.000000  1332.000000   \n",
       "mean        0.47973         4.939762    19.704204     1.222973   \n",
       "std         1.27168         0.231071    19.202731     1.847446   \n",
       "min         0.00000         2.231322     0.000000     0.000000   \n",
       "25%         0.00000         4.791679    13.000000     0.000000   \n",
       "50%         0.00000         4.946059    18.000000     1.000000   \n",
       "75%         0.00000         5.092938    24.000000     2.000000   \n",
       "max        26.00000         5.681429   642.000000    17.000000   \n",
       "\n",
       "       avg_word_sentence          POS  POS/total_words  prompt_words  \\\n",
       "count        1332.000000  1332.000000      1332.000000   1332.000000   \n",
       "mean           23.884687   420.596542         0.989935    198.913664   \n",
       "std            11.160020   170.985111         0.007308     82.729266   \n",
       "min             1.084112    35.647059         0.924771     14.000000   \n",
       "25%            19.142857   305.406284         0.987758    144.000000   \n",
       "50%            22.030331   406.982869         0.991572    193.000000   \n",
       "75%            26.048234   520.739458         0.994425    246.000000   \n",
       "max           303.000000  1158.984563         1.000000    669.000000   \n",
       "\n",
       "       prompt_words/total_words  synonym_words  synonym_words/total_words  \\\n",
       "count               1332.000000     1332.00000                1332.000000   \n",
       "mean                   0.469164      110.16967                   0.263846   \n",
       "std                    0.052466       43.96192                   0.038870   \n",
       "min                    0.288889       11.00000                   0.027299   \n",
       "25%                    0.435709       81.00000                   0.238423   \n",
       "50%                    0.465852      107.50000                   0.262872   \n",
       "75%                    0.500000      134.00000                   0.288277   \n",
       "max                    0.961207      355.00000                   0.465517   \n",
       "\n",
       "         unstemmed      stemmed        score  \n",
       "count  1332.000000  1332.000000  1332.000000  \n",
       "mean    468.987988   455.507508     3.427177  \n",
       "std     159.447449   155.751220     0.774275  \n",
       "min      48.000000    50.000000     1.000000  \n",
       "25%     361.000000   350.750000     3.000000  \n",
       "50%     463.000000   448.000000     3.000000  \n",
       "75%     581.000000   561.250000     4.000000  \n",
       "max     750.000000   750.000000     6.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate descriptive statistics for the essay_data DataFrame\n",
    "essay_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essayid                      277399.827726\n",
       "chars                        749893.216171\n",
       "words                         29540.579060\n",
       "commas                          119.263460\n",
       "apostrophes                      37.509741\n",
       "punctuations                      1.617170\n",
       "avg_word_length                   0.053394\n",
       "sentences                       368.744896\n",
       "questions                         3.413056\n",
       "avg_word_sentence               124.546047\n",
       "POS                           29235.908019\n",
       "POS/total_words                   0.000053\n",
       "prompt_words                   6844.131534\n",
       "prompt_words/total_words          0.002753\n",
       "synonym_words                  1932.650379\n",
       "synonym_words/total_words         0.001511\n",
       "unstemmed                     25423.488962\n",
       "stemmed                       24258.442468\n",
       "score                             0.599501\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the variance for each column\n",
    "essay_data.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Machine Learning** is a subcategory of machine learning which involves training an algorithm on data that is well labeled, i.e. labeled training data. The goal is to obtain an approximation of the function that maps the input to the output so that when it is provided with new input data, it is able to predict the output for that data (IBM Cloud Education, 2020; Brownlee, 2020).\n",
    "\n",
    "In the context of machine learning, **labeled data** refers to data that has one or more meaningful labels or informative tags that can be used by supervised machine learning models to learn the combination of features that might map the input to that label, and thus be able to predict labels for unlabelled data. For example, given a picture the label might indicate whether the picture contains a cat or not (Amazon Web Services, n.d.).\n",
    "\n",
    "In machine learning, the original dataset is usually divided into the **training set** and the **test set**, with the training set typically many times larger than the test set (usually an 80/20 split), where the training set is a subset of the dataset that is used to train a model, i.e. it is a set of examples used to fit the parameters/weights of a model, and the test set is a subset of the dataset that is not used during training and is instead used to test the performance of the trained model (Google Developers, 2020). Splitting the dataset in this way is required to avoid overfitting where the model performs very well with the provided data but performs very poorly when given new data, in this sense the test set serves as a proxy for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Separating Features and Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dataset, we can surmise that the label, i.e. what we want the model to predict, are the **scores** which represents the grade of the essay, therefore we can obtain the data labels by extracting the values from the *score* column.\n",
    "\n",
    "Here we extract the values from the last column (*score*), and assign them to the variable *label* to be used later on during preprocessing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values from every row, last column\n",
    "label = essay_data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the values from the first column (*essayid*) to the second last column (*stemmed*) and assign these values to the variable *features* to be used during preprocessing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values from every row, from first to second last column (i.e. columns index 0 to 17)\n",
    "features = essay_data.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the data into the training set and the test set, we can perform feature selection to identify and select the features that will contribute most to the model's prediction. This is done to increase the model's accuracy, decrease training time, and reduce overfitting. \n",
    "\n",
    "To do this we can make use of the *f_classif()* function from the *sklearn.feature_selection* module that computes the ANOVA F-value for the provided sample and allows for feature selection when we are working with a classification problem, more specifically, when we have numerical input data (features) and a categorical target variable (label) as we do in this case.\n",
    "\n",
    "Here we can print out the F-value for each of the (feature) columns to determine which columns to keep, the higher the score the more relevant the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----List of features from highest to lowest F-value----\n",
      "\n",
      "unstemmed = 255.8948587663438\n",
      "stemmed = 254.33217075322452\n",
      "chars = 236.13263126536944\n",
      "POS = 209.80140389791052\n",
      "words = 208.92684234127228\n",
      "prompt_words = 186.39330700069752\n",
      "synonym_words = 135.72391926093715\n",
      "commas = 106.40189465413823\n",
      "POS/total_words = 33.979754886876925\n",
      "apostrophes = 33.93409241646121\n",
      "questions = 32.777162227254095\n",
      "avg_word_length = 32.4444185987326\n",
      "synonym_words/total_words = 28.505876406086006\n",
      "sentences = 15.93339650670346\n",
      "punctuations = 7.439281205003285\n",
      "avg_word_sentence = 5.065326414935797\n",
      "prompt_words/total_words = 1.892359146707649\n",
      "essayid = 1.1098589592655128\n"
     ]
    }
   ],
   "source": [
    "# the f_classif() function takes in the features and label data and returns the F-values for each feature\n",
    "F_vals, _ = f_classif(features, label)\n",
    "\n",
    "# gets the list of (feature) column names\n",
    "cols = essay_data.columns[:-1]\n",
    "# sort based on the F-values in descending order\n",
    "ind = np.lexsort((cols,F_vals))[::-1]\n",
    "\n",
    "print(\"----List of features from highest to lowest F-value----\\n\")\n",
    "# for each feature, display the column (feature) name along with its F-value\n",
    "for index in ind:\n",
    "    print(\"{} = {}\".format(cols[index], F_vals[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select the features which are relevant in predicting the essay's grade by defining a threshold of, say, 100 to extract the features that are most important. From the list above we can see that there are 8 features with an F-value above 100: *unstemmed, stemmed, chars, POS, words, prompt_words, synonym_words, commas*.\n",
    "\n",
    "We can now extract these 8 features and from this point onwards we will only be working and training with the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the column headers of the features we want to keep\n",
    "selected_cols = ['unstemmed', 'stemmed', 'chars', 'POS', 'words', 'prompt_words', 'synonym_words', 'commas']\n",
    "# extract the values only from the selected columns\n",
    "selected_features = essay_data[selected_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "After feature selection, we can now split the data into the training set and the test set by using the *train_test_split()* function from the *sklearn.model_selection* module. Typically the ratio of the size of the training set to the size of the test set is 80:20 so here we will follow this convention. This function returns four arrays listed below in order:\n",
    "- **X_train** : an array of values which represents the feature values for examples which are going to be used to train the model (part of the training set)\n",
    "- **X_test** : an array of values which represents the feature values for examples which are going to be used to test the accuracy of the model after it has been trained (part of the test set)\n",
    "- **y_train** : an array of values which represents the label values for examples which are going to be used to train the model (part of the training set)\n",
    "- **y_test** : an array of values which represents the label values for examples which are going to be used to test the accuracy of the model after it has been trained by comparing it to the model's predictions (part of the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first parameter is the (selected) feature values\n",
    "# second parameter is the label values\n",
    "# test_size parameter takes a float between 0 and 1, which represents the proportion of the test set to the entire dataset\n",
    "# random_state parameter takes in an int so that the output is reproducible\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the split was done correctly by printing the shape of X_train, the shape of X_test, and the proportion of data in the training set to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 1065 rows and 8 columns\n",
      "X_test has 267 rows and 8 columns\n",
      "proportion of data in the X_test to the entire dataset: 0.20045045045045046\n"
     ]
    }
   ],
   "source": [
    "train_rows, train_cols = X_train.shape\n",
    "test_rows, test_cols = X_test.shape\n",
    "print(\"X_train has {} rows and {} columns\".format(train_rows, train_cols))\n",
    "print(\"X_test has {} rows and {} columns\".format(test_rows, test_cols))\n",
    "print(\"proportion of data in the X_test to the entire dataset:\", test_rows/essay_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Difference between binary and multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification refers to a predictive modeling problem that involves predicting a class label for some input data. Two of the most common types of these problems are binary and multi-class classification. \n",
    "\n",
    "In binary classification, there are only two class labels, therefore given some input data the model has to predict whether it is of some class X or not. For example, given an image the model has to predict whether it is a cat or not. It only requires one classifier model. (Band, 2020)\n",
    "\n",
    "Whereas in multi-class classification, there are multiple class labels, therefore, given some input data the model has to assign it to one of these labels. It might require more than one classifier model (Band, 2020). Since we have determined that the *score* column contains values that are categorical, and because there is more than two possible labels (scores can be one of [1,2,3,4,5,6]), in this case, predicting the score for an essay is an example of a multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization/Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization/Scaling is an important step in preprocessing as it changes the values in the numeric columns to a common scale while keeping the relative differences in the range of values (Jaitley, 2018). This is especially important when different features have vastly different ranges. For example, the *chars* column contains very large values when compared to the values in the *punctuations* column. The values in the *chars* column might then have a greater influence over the result due to its larger magnitude even though it might not necessarily be a better predictor than the values in the *punctuations* column. Therefore by normalizing the range of all features we can avoid this problem.\n",
    "\n",
    "Since we are working with features that have very different ranges (for example *chars* and *punctuations*) and we are planning to use an SVM, which assumes that the data provided is in a standard range, therefore normalization/scaling is required.\n",
    "\n",
    "Here we can use the *MinMaxScaler()* function from the *sklearn.preprocessing* module to scale each feature to the range [0,1]. Not only does the MinMaxScaler preserve the shape of the original distribution, it also avoids significantly altering the information embedded in the original data (Hale, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scaler we are going to use as the MinMaxScaler\n",
    "# we are going to scale the features to the default range of [0,1] since we have no negative values\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the scaler to the training data which computes the minimum and maximum of every feature in X_train\n",
    "# then we transform X_train to scale the data according to the computed minimum and maximum \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Here we transform X_test using the minimum and maximum of the corresponding features in X_train\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this by printing out the maximum and minimum values for every feature in both the training and test sets. Note that while the minimum and maximum values of the features in the training set are 0s and 1s respectively, the minimum and maximum values of the features in the test set are not, with some of the maximum feature values exceeding 1. This is to be expected because the scaler was fitted according to the **training data** not the **test data**, what is important here is that the minimum and maximum feature values for the test set is still very close to the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum values for each feature in the training set: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Maximum values for each feature in the training set: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Minimum values for each feature in the test set: [0.01709402 0.01571429 0.00070909 0.00032579 0.0018315  0.\n",
      " 0.         0.        ]\n",
      "Maximum values for each feature in the test set: [1.         1.         1.05885481 1.03691839 1.03846154 1.2405303\n",
      " 1.17006803 0.68055556]\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum values for each feature in the training set:\",X_train.min(axis=0))\n",
    "print(\"Maximum values for each feature in the training set:\", X_train.max(axis=0))\n",
    "print(\"Minimum values for each feature in the test set:\", X_test.min(axis=0))\n",
    "print(\"Maximum values for each feature in the test set:\", X_test.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;SVM in Relation to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Linear Regression, Support Vector Machines (SVMs) are supervised machine learning models. Whereas linear regression is typically used for regression problems, SVMs can be used for both classification *and* regression tasks (Gandhi, 2018). In regression tasks, the goal of linear regression is to minimize the sum of the squared errors, however the goal of the SVM (SVR) is to instead minimize the l2-norm of the coefficient vector, not the squared error which is instead handled in the constraints. This means that the SVM (SVR) gives us more flexibility as it allows us to ignore errors as long as they fall within an acceptable range/margin (Sharp, 2020).\n",
    "\n",
    "In classification tasks, the goal of an SVM algorithm is to find the optimal hyperplane that is able to best classify the input data. To this end, many hyperplanes are tested and the plane that has the maximum margin, i.e. the hyperplane that results in the maximum distance between the closest points from separate classes, is chosen (Gandhi, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels or sometimes known as the \"kernel trick\" allows SVMs to create non-linear decision boundaries. These kernels are mathematical functions which takes input data in n dimensional space, which is not separable using a hyperplane, and transforms them into a higher dimension in which this data *is* separable. The resulting hyperplane can then be projected back down into n dimensional space to get a non-linear decision boundary (Suriya, n.d.). There are a couple of kernels for example linear, polynomial, radial basis function (rbf), and sigmoid but the most commonly used ones are linear and rbf (DataFlair, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; Building the SVM (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will be using SVM for classification, I will use the *SVC* (Support Vector Classification) class from the *sklearn.svm* module. In order to build a good model, we need to choose optimal parameters for the model. For the *SVC* class, the most important parameters are:\n",
    "- kernel: the type of kernel to be used in the algorithm\n",
    "- C: the regularization parameter which tells the algorithm how important misclassified points are\n",
    "- gamma: the kernel coefficient, it decides the level of influence of a single example\n",
    "\n",
    "Rather than randomly guessing values for the parameters, we can use the *GridSearchCV* class from the *sklearn.model_selection* module. GridSearchCV allows us to test every combination of the provided values for each of these parameters to find the parameters which result in the highest performing model when it is fitted to the training set.\n",
    "\n",
    "Here I allowed it to choose between two kernels - *linear* and *rbf*, as they are the most commonly used and reliable kernels, whereas for C and gamma I provided a wider range of values to test with. \n",
    "\n",
    "*Note that gamma is a parameter that is ignored by linear kernels therefore to reduce search time I only allowed GridSearchCV to test the different gamma values when the kernel being tested is the rbf kernel.*\n",
    "\n",
    "Running the code block below should only take ~10-30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n",
      "[CV 1/5; 1/35] START C=0.1, kernel=linear.......................................\n",
      "[CV 1/5; 1/35] END .....................C=0.1, kernel=linear; total time=   0.0s\n",
      "[CV 2/5; 1/35] START C=0.1, kernel=linear.......................................\n",
      "[CV 2/5; 1/35] END .....................C=0.1, kernel=linear; total time=   0.0s\n",
      "[CV 3/5; 1/35] START C=0.1, kernel=linear.......................................\n",
      "[CV 3/5; 1/35] END .....................C=0.1, kernel=linear; total time=   0.0s\n",
      "[CV 4/5; 1/35] START C=0.1, kernel=linear.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/35] END .....................C=0.1, kernel=linear; total time=   0.0s\n",
      "[CV 5/5; 1/35] START C=0.1, kernel=linear.......................................\n",
      "[CV 5/5; 1/35] END .....................C=0.1, kernel=linear; total time=   0.0s\n",
      "[CV 1/5; 2/35] START C=1, kernel=linear.........................................\n",
      "[CV 1/5; 2/35] END .......................C=1, kernel=linear; total time=   0.0s\n",
      "[CV 2/5; 2/35] START C=1, kernel=linear.........................................\n",
      "[CV 2/5; 2/35] END .......................C=1, kernel=linear; total time=   0.0s\n",
      "[CV 3/5; 2/35] START C=1, kernel=linear.........................................\n",
      "[CV 3/5; 2/35] END .......................C=1, kernel=linear; total time=   0.0s\n",
      "[CV 4/5; 2/35] START C=1, kernel=linear.........................................\n",
      "[CV 4/5; 2/35] END .......................C=1, kernel=linear; total time=   0.0s\n",
      "[CV 5/5; 2/35] START C=1, kernel=linear.........................................\n",
      "[CV 5/5; 2/35] END .......................C=1, kernel=linear; total time=   0.0s\n",
      "[CV 1/5; 3/35] START C=10, kernel=linear........................................\n",
      "[CV 1/5; 3/35] END ......................C=10, kernel=linear; total time=   0.0s\n",
      "[CV 2/5; 3/35] START C=10, kernel=linear........................................\n",
      "[CV 2/5; 3/35] END ......................C=10, kernel=linear; total time=   0.0s\n",
      "[CV 3/5; 3/35] START C=10, kernel=linear........................................\n",
      "[CV 3/5; 3/35] END ......................C=10, kernel=linear; total time=   0.0s\n",
      "[CV 4/5; 3/35] START C=10, kernel=linear........................................\n",
      "[CV 4/5; 3/35] END ......................C=10, kernel=linear; total time=   0.0s\n",
      "[CV 5/5; 3/35] START C=10, kernel=linear........................................\n",
      "[CV 5/5; 3/35] END ......................C=10, kernel=linear; total time=   0.0s\n",
      "[CV 1/5; 4/35] START C=100, kernel=linear.......................................\n",
      "[CV 1/5; 4/35] END .....................C=100, kernel=linear; total time=   0.0s\n",
      "[CV 2/5; 4/35] START C=100, kernel=linear.......................................\n",
      "[CV 2/5; 4/35] END .....................C=100, kernel=linear; total time=   0.0s\n",
      "[CV 3/5; 4/35] START C=100, kernel=linear.......................................\n",
      "[CV 3/5; 4/35] END .....................C=100, kernel=linear; total time=   0.0s\n",
      "[CV 4/5; 4/35] START C=100, kernel=linear.......................................\n",
      "[CV 4/5; 4/35] END .....................C=100, kernel=linear; total time=   0.0s\n",
      "[CV 5/5; 4/35] START C=100, kernel=linear.......................................\n",
      "[CV 5/5; 4/35] END .....................C=100, kernel=linear; total time=   0.0s\n",
      "[CV 1/5; 5/35] START C=1000, kernel=linear......................................\n",
      "[CV 1/5; 5/35] END ....................C=1000, kernel=linear; total time=   2.2s\n",
      "[CV 2/5; 5/35] START C=1000, kernel=linear......................................\n",
      "[CV 2/5; 5/35] END ....................C=1000, kernel=linear; total time=   0.8s\n",
      "[CV 3/5; 5/35] START C=1000, kernel=linear......................................\n",
      "[CV 3/5; 5/35] END ....................C=1000, kernel=linear; total time=   0.8s\n",
      "[CV 4/5; 5/35] START C=1000, kernel=linear......................................\n",
      "[CV 4/5; 5/35] END ....................C=1000, kernel=linear; total time=   0.1s\n",
      "[CV 5/5; 5/35] START C=1000, kernel=linear......................................\n",
      "[CV 5/5; 5/35] END ....................C=1000, kernel=linear; total time=   0.1s\n",
      "[CV 1/5; 6/35] START C=0.1, gamma=0.0001, kernel=rbf............................\n",
      "[CV 1/5; 6/35] END ..........C=0.1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 6/35] START C=0.1, gamma=0.0001, kernel=rbf............................\n",
      "[CV 2/5; 6/35] END ..........C=0.1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 6/35] START C=0.1, gamma=0.0001, kernel=rbf............................\n",
      "[CV 3/5; 6/35] END ..........C=0.1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 6/35] START C=0.1, gamma=0.0001, kernel=rbf............................\n",
      "[CV 4/5; 6/35] END ..........C=0.1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 6/35] START C=0.1, gamma=0.0001, kernel=rbf............................\n",
      "[CV 5/5; 6/35] END ..........C=0.1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 7/35] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 1/5; 7/35] END ...........C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 7/35] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 7/35] END ...........C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 7/35] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 7/35] END ...........C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 7/35] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 7/35] END ...........C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 7/35] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 7/35] END ...........C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 8/35] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 1/5; 8/35] END ............C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 8/35] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 2/5; 8/35] END ............C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 8/35] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 3/5; 8/35] END ............C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 8/35] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 4/5; 8/35] END ............C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 8/35] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 5/5; 8/35] END ............C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 9/35] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 1/5; 9/35] END .............C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 9/35] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 2/5; 9/35] END .............C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 9/35] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 3/5; 9/35] END .............C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 9/35] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 4/5; 9/35] END .............C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 9/35] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 5/5; 9/35] END .............C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 10/35] START C=0.1, gamma=1, kernel=rbf................................\n",
      "[CV 1/5; 10/35] END ..............C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 10/35] START C=0.1, gamma=1, kernel=rbf................................\n",
      "[CV 2/5; 10/35] END ..............C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 10/35] START C=0.1, gamma=1, kernel=rbf................................\n",
      "[CV 3/5; 10/35] END ..............C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 10/35] START C=0.1, gamma=1, kernel=rbf................................\n",
      "[CV 4/5; 10/35] END ..............C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 10/35] START C=0.1, gamma=1, kernel=rbf................................\n",
      "[CV 5/5; 10/35] END ..............C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 11/35] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 1/5; 11/35] END .............C=0.1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 11/35] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 2/5; 11/35] END .............C=0.1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 11/35] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 3/5; 11/35] END .............C=0.1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 11/35] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 4/5; 11/35] END .............C=0.1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 11/35] START C=0.1, gamma=10, kernel=rbf...............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 11/35] END .............C=0.1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 12/35] START C=1, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 1/5; 12/35] END ...........C=1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 12/35] START C=1, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 2/5; 12/35] END ...........C=1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 12/35] START C=1, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 3/5; 12/35] END ...........C=1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 12/35] START C=1, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 4/5; 12/35] END ...........C=1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 12/35] START C=1, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 5/5; 12/35] END ...........C=1, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 13/35] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 1/5; 13/35] END ............C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 13/35] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 2/5; 13/35] END ............C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 13/35] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 3/5; 13/35] END ............C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 13/35] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 4/5; 13/35] END ............C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 13/35] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 5/5; 13/35] END ............C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 14/35] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 1/5; 14/35] END .............C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 14/35] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 2/5; 14/35] END .............C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 14/35] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 3/5; 14/35] END .............C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 14/35] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 4/5; 14/35] END .............C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 14/35] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 5/5; 14/35] END .............C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 15/35] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 1/5; 15/35] END ..............C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 15/35] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 2/5; 15/35] END ..............C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 15/35] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 3/5; 15/35] END ..............C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 15/35] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 4/5; 15/35] END ..............C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 15/35] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 5/5; 15/35] END ..............C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 16/35] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 1/5; 16/35] END ................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 16/35] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 2/5; 16/35] END ................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 16/35] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 3/5; 16/35] END ................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 16/35] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 4/5; 16/35] END ................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 16/35] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 5/5; 16/35] END ................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 17/35] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 1/5; 17/35] END ...............C=1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 17/35] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 2/5; 17/35] END ...............C=1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 17/35] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 3/5; 17/35] END ...............C=1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 17/35] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 4/5; 17/35] END ...............C=1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 17/35] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 5/5; 17/35] END ...............C=1, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 18/35] START C=10, gamma=0.0001, kernel=rbf............................\n",
      "[CV 1/5; 18/35] END ..........C=10, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 18/35] START C=10, gamma=0.0001, kernel=rbf............................\n",
      "[CV 2/5; 18/35] END ..........C=10, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 18/35] START C=10, gamma=0.0001, kernel=rbf............................\n",
      "[CV 3/5; 18/35] END ..........C=10, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 18/35] START C=10, gamma=0.0001, kernel=rbf............................\n",
      "[CV 4/5; 18/35] END ..........C=10, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 18/35] START C=10, gamma=0.0001, kernel=rbf............................\n",
      "[CV 5/5; 18/35] END ..........C=10, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 19/35] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 1/5; 19/35] END ...........C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 19/35] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 19/35] END ...........C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 19/35] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 19/35] END ...........C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 19/35] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 19/35] END ...........C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 19/35] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 19/35] END ...........C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 20/35] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 1/5; 20/35] END ............C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 20/35] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 2/5; 20/35] END ............C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 20/35] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 3/5; 20/35] END ............C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 20/35] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 4/5; 20/35] END ............C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 20/35] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 5/5; 20/35] END ............C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 21/35] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 1/5; 21/35] END .............C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 21/35] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 2/5; 21/35] END .............C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 21/35] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 3/5; 21/35] END .............C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 21/35] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 4/5; 21/35] END .............C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 21/35] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 5/5; 21/35] END .............C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 22/35] START C=10, gamma=1, kernel=rbf.................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 22/35] END ...............C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 22/35] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 2/5; 22/35] END ...............C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 22/35] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 3/5; 22/35] END ...............C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 22/35] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 4/5; 22/35] END ...............C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 22/35] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 5/5; 22/35] END ...............C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 23/35] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 1/5; 23/35] END ..............C=10, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 23/35] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 2/5; 23/35] END ..............C=10, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 23/35] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 3/5; 23/35] END ..............C=10, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 23/35] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 4/5; 23/35] END ..............C=10, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 23/35] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 5/5; 23/35] END ..............C=10, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 24/35] START C=100, gamma=0.0001, kernel=rbf...........................\n",
      "[CV 1/5; 24/35] END .........C=100, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 24/35] START C=100, gamma=0.0001, kernel=rbf...........................\n",
      "[CV 2/5; 24/35] END .........C=100, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 24/35] START C=100, gamma=0.0001, kernel=rbf...........................\n",
      "[CV 3/5; 24/35] END .........C=100, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 24/35] START C=100, gamma=0.0001, kernel=rbf...........................\n",
      "[CV 4/5; 24/35] END .........C=100, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 24/35] START C=100, gamma=0.0001, kernel=rbf...........................\n",
      "[CV 5/5; 24/35] END .........C=100, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 25/35] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 1/5; 25/35] END ..........C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 25/35] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 2/5; 25/35] END ..........C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 25/35] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 3/5; 25/35] END ..........C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 25/35] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 4/5; 25/35] END ..........C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 25/35] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 5/5; 25/35] END ..........C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 26/35] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 1/5; 26/35] END ...........C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 26/35] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 2/5; 26/35] END ...........C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 26/35] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 3/5; 26/35] END ...........C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 26/35] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 4/5; 26/35] END ...........C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 26/35] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 5/5; 26/35] END ...........C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 27/35] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 1/5; 27/35] END ............C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 27/35] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 2/5; 27/35] END ............C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 27/35] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 3/5; 27/35] END ............C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 27/35] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 4/5; 27/35] END ............C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 27/35] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 5/5; 27/35] END ............C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 28/35] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 1/5; 28/35] END ..............C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 28/35] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 2/5; 28/35] END ..............C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 28/35] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 3/5; 28/35] END ..............C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 28/35] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 4/5; 28/35] END ..............C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 28/35] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 5/5; 28/35] END ..............C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 29/35] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 1/5; 29/35] END .............C=100, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 29/35] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 2/5; 29/35] END .............C=100, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 29/35] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 3/5; 29/35] END .............C=100, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 29/35] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 4/5; 29/35] END .............C=100, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 29/35] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 5/5; 29/35] END .............C=100, gamma=10, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 30/35] START C=1000, gamma=0.0001, kernel=rbf..........................\n",
      "[CV 1/5; 30/35] END ........C=1000, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 30/35] START C=1000, gamma=0.0001, kernel=rbf..........................\n",
      "[CV 2/5; 30/35] END ........C=1000, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 30/35] START C=1000, gamma=0.0001, kernel=rbf..........................\n",
      "[CV 3/5; 30/35] END ........C=1000, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 30/35] START C=1000, gamma=0.0001, kernel=rbf..........................\n",
      "[CV 4/5; 30/35] END ........C=1000, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 30/35] START C=1000, gamma=0.0001, kernel=rbf..........................\n",
      "[CV 5/5; 30/35] END ........C=1000, gamma=0.0001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 31/35] START C=1000, gamma=0.001, kernel=rbf...........................\n",
      "[CV 1/5; 31/35] END .........C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 31/35] START C=1000, gamma=0.001, kernel=rbf...........................\n",
      "[CV 2/5; 31/35] END .........C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 31/35] START C=1000, gamma=0.001, kernel=rbf...........................\n",
      "[CV 3/5; 31/35] END .........C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 31/35] START C=1000, gamma=0.001, kernel=rbf...........................\n",
      "[CV 4/5; 31/35] END .........C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 31/35] START C=1000, gamma=0.001, kernel=rbf...........................\n",
      "[CV 5/5; 31/35] END .........C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 32/35] START C=1000, gamma=0.01, kernel=rbf............................\n",
      "[CV 1/5; 32/35] END ..........C=1000, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 32/35] START C=1000, gamma=0.01, kernel=rbf............................\n",
      "[CV 2/5; 32/35] END ..........C=1000, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 32/35] START C=1000, gamma=0.01, kernel=rbf............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 32/35] END ..........C=1000, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 32/35] START C=1000, gamma=0.01, kernel=rbf............................\n",
      "[CV 4/5; 32/35] END ..........C=1000, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 32/35] START C=1000, gamma=0.01, kernel=rbf............................\n",
      "[CV 5/5; 32/35] END ..........C=1000, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 33/35] START C=1000, gamma=0.1, kernel=rbf.............................\n",
      "[CV 1/5; 33/35] END ...........C=1000, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 33/35] START C=1000, gamma=0.1, kernel=rbf.............................\n",
      "[CV 2/5; 33/35] END ...........C=1000, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 3/5; 33/35] START C=1000, gamma=0.1, kernel=rbf.............................\n",
      "[CV 3/5; 33/35] END ...........C=1000, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 4/5; 33/35] START C=1000, gamma=0.1, kernel=rbf.............................\n",
      "[CV 4/5; 33/35] END ...........C=1000, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 5/5; 33/35] START C=1000, gamma=0.1, kernel=rbf.............................\n",
      "[CV 5/5; 33/35] END ...........C=1000, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 34/35] START C=1000, gamma=1, kernel=rbf...............................\n",
      "[CV 1/5; 34/35] END .............C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 2/5; 34/35] START C=1000, gamma=1, kernel=rbf...............................\n",
      "[CV 2/5; 34/35] END .............C=1000, gamma=1, kernel=rbf; total time=   0.1s\n",
      "[CV 3/5; 34/35] START C=1000, gamma=1, kernel=rbf...............................\n",
      "[CV 3/5; 34/35] END .............C=1000, gamma=1, kernel=rbf; total time=   0.1s\n",
      "[CV 4/5; 34/35] START C=1000, gamma=1, kernel=rbf...............................\n",
      "[CV 4/5; 34/35] END .............C=1000, gamma=1, kernel=rbf; total time=   0.1s\n",
      "[CV 5/5; 34/35] START C=1000, gamma=1, kernel=rbf...............................\n",
      "[CV 5/5; 34/35] END .............C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV 1/5; 35/35] START C=1000, gamma=10, kernel=rbf..............................\n",
      "[CV 1/5; 35/35] END ............C=1000, gamma=10, kernel=rbf; total time=   0.3s\n",
      "[CV 2/5; 35/35] START C=1000, gamma=10, kernel=rbf..............................\n",
      "[CV 2/5; 35/35] END ............C=1000, gamma=10, kernel=rbf; total time=   0.2s\n",
      "[CV 3/5; 35/35] START C=1000, gamma=10, kernel=rbf..............................\n",
      "[CV 3/5; 35/35] END ............C=1000, gamma=10, kernel=rbf; total time=   0.2s\n",
      "[CV 4/5; 35/35] START C=1000, gamma=10, kernel=rbf..............................\n",
      "[CV 4/5; 35/35] END ............C=1000, gamma=10, kernel=rbf; total time=   0.2s\n",
      "[CV 5/5; 35/35] START C=1000, gamma=10, kernel=rbf..............................\n",
      "[CV 5/5; 35/35] END ............C=1000, gamma=10, kernel=rbf; total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(random_state=42),\n",
       "             param_grid=[{'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [0.1, 1, 10, 100, 1000],\n",
       "                          'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
       "                          'kernel': ['rbf']}],\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the SVC\n",
    "# provide an integer for random_state so that the output is reproducible\n",
    "svm = SVC(random_state = 42)\n",
    "\n",
    "# lists the possible values for the 3 important parameters - kernel, C, and gamma\n",
    "parameters = [\n",
    "    {'C' : [0.1, 1, 10, 100, 1000], 'kernel' : ['linear']},\n",
    "    {'C' : [0.1, 1, 10, 100, 1000], 'gamma' : [0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel' : [ 'rbf']}\n",
    "]\n",
    "\n",
    "# in the first parameter the estimator for which we want to test out the parameter combinations is passed in\n",
    "# In the second parameter, try every combination of parameter values for the provided estimator (svm)\n",
    "# verbose parameter controls its verbosity, a value of 1 just displays the total number of combinations\n",
    "opt_svm = GridSearchCV(svm, parameters, verbose=10)\n",
    "\n",
    "# fit the svm to the training set and try all the parameter combinations to find the best one\n",
    "opt_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been fitted to the training set and the best parameters have been chosen, we can see which of the parameter values had the best result by accessing the *best_params_* attribute of GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_svm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that among the provided values for each parameter, a C value of 1000, a gamma of 1, and an rbf kernel performed the best with the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained SVM model, we can use it to predict the scores for the essays in the training set. We can do this by calling the *.predict()* function of the trained model and passing in the feature values of the essays in the test set. This results in an array of predicted scores for each of the essays in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions of the essays in the test set\n",
    "predictions = opt_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the predictions by simply printing out the *predictions* array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3, 2, 3, 4, 3, 4, 2, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3,\n",
       "       3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 3, 3, 4, 3, 4, 3, 2, 4, 4, 4,\n",
       "       4, 3, 3, 2, 4, 3, 4, 3, 4, 4, 4, 2, 4, 3, 4, 4, 4, 3, 5, 4, 3, 5,\n",
       "       3, 4, 3, 3, 3, 4, 3, 2, 4, 3, 4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4,\n",
       "       2, 3, 4, 3, 4, 4, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 3,\n",
       "       4, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 2, 4, 3, 3, 4, 4, 3, 4, 3, 3, 4,\n",
       "       4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3,\n",
       "       3, 1, 4, 4, 2, 3, 4, 3, 4, 2, 3, 4, 3, 3, 4, 3, 4, 3, 4, 3, 4, 4,\n",
       "       3, 4, 3, 4, 3, 3, 2, 3, 3, 4, 4, 3, 3, 2, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 4, 2, 3,\n",
       "       3, 3, 4, 3, 4, 3, 3, 3, 3, 2, 3, 4, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4,\n",
       "       2, 4, 4, 4, 4, 3, 2, 3, 3, 3, 4, 3, 4, 4, 4, 1, 3, 3, 4, 3, 3, 4,\n",
       "       3, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that they are valid scores/predictions we can find the minimum and maximum score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum score predicted:  1\n",
      "Maximum score predicted:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum score predicted: \", predictions.min())\n",
    "print(\"Maximum score predicted: \", predictions.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that indeed the array consists of integers between 1 to 5 which is within the range of possible scores (1 to 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are evaluating a classification model, we can use a confusion matrix which allows for the visualization of the model's performance by displaying a grid of the predicted labels against the actual labels.\n",
    "\n",
    "The first step is to compute the confusion matrix using the *confusion_matrix()* function from the *sklearn.metrics* module and pass in the actual labels along with the predicted labels. Then we can use the *ConfusionMatrixDisplay* class from the same module for the visual representation of this confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23681463ca0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the confusion matrix using the actual scores of the essays in the test set and the model predictions\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "# first parameter is the confusion matrix computed\n",
    "# display_labels parameter defines the list of possible labels (from 1 to 6), since essay scores range from 1-6\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[1,2,3,4,5,6])\n",
    "\n",
    "# display the confusion matrix\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the confusion matrix, the value in row i column j represents the number of essays for which the model predicted a score of j but actually has a score of i. For example, the value in row 1 column 2 is 3. This means that there are 3 essays for which the model predicted a score of 2 but in actuality, they had a score of 1.\n",
    "\n",
    "The most important thing to notice in the grid above are the values in the diagonal from the upper left to the lower right as they represent the number of predictions that are correct, the brighter the square the higher the frequency. Ideally this diagonal should be lit up. Notice here that aside from row 3 column 3, and row 4 column 4, all the other squares on this diagonal are quite dark however this does not necessarily mean that the model's performance is poor, in this case it simply means that there are comparably few essays with scores less than 3 or more than 4. For example, if we total up the values in row 6 we can see that there is only 1 essay that has a score of 6!\n",
    "\n",
    "Of course this means that values *not* on the diagonal should ideally be very dark as they represent wrong predictions.\n",
    "\n",
    "Therefore since the (middle of the) diagonal is well-lit and the squares that are *not* on the diagonal are mostly dark we can get some sense that the model's performance is quite good. We can reinforce this by testing the model with the QWK metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;&emsp;QWK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quadratic Weighted Kappa (QWK) is a metric that is used to evaluate the amount of agreement between the model or algorithm's predictions and the actual labels and tries to account for random chance by adjusting for the probability that the model's prediction agrees with the actual label \"by chance\". It does this by assuming that there is a predetermined proportion of objects assigned to each label. Most notably this metric is used to score submissions on *kaggle* (Arora, 2019).\n",
    "\n",
    "The possible scores resulting from this metric ranges from a minimum of -1.0 to a maximum score of 1.0, and the aim is to get as close to 1.0 as possible (Arora, 2019).\n",
    "\n",
    "We can compute the Quadratic Weighted Kappa metric by using the *cohen_kappa_score()* function in the *sklearn.metrics* module with the *weights* parameter set as 'quadratic'. In the code block below we are comparing the model's predictions with the actual scores (labels) to compute a score denoting the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133519059590949"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the QWK for the model predictions\n",
    "cohen_kappa_score(y_test, predictions, weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest possible score is 1.0 which denotes that the model's predictions are exactly the same as the actual labels, a score of 0.0 indicates that the model's predictions which do agree with the actual labels simply match \"by chance\". On the other hand, a score of -1.0 indicates that the model's predictions are as far away as possible from the actual labels (Arora, 2019).\n",
    "\n",
    "Typically, a score of 0.6+ is considered very good (Arora, 2019), therefore since we can see from the output of the code above that our model has a score of 0.71 we can now be confident that it is quite a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the kaggle submission, I decided to use a random forest classifier as I found that it produces better results compared to using an SVM. My final score (kaggle) is **0.72775**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp; Using the SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used the SVM model to output the predictions of the unlabeled data *'FIT1043-Essay-Features-Submission.csv'* into a formatted CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; Reading File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to read the *'FIT1043-Essay-Features-Submission.csv'* file and convert it to a DataFrame by using the *.read_csv()* function from the *pandas* library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = pd.read_csv('FIT1043-Essay-Features-Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the data that we have read to get a glimpse at the data. The most notable thing here is that it differs from the original dataset as it is missing the score column, this is because we are meant to fill this column with the predictions of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4332</td>\n",
       "      <td>900</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.813333</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>893.988852</td>\n",
       "      <td>0.993321</td>\n",
       "      <td>392</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>196</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>1465</td>\n",
       "      <td>280</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.232143</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>278.321343</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>131</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>51</td>\n",
       "      <td>0.182143</td>\n",
       "      <td>339</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>1696</td>\n",
       "      <td>325</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.218462</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17.105263</td>\n",
       "      <td>321.316770</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>178</td>\n",
       "      <td>0.547692</td>\n",
       "      <td>92</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>352</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>2640</td>\n",
       "      <td>555</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4.756757</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.821429</td>\n",
       "      <td>551.989150</td>\n",
       "      <td>0.994575</td>\n",
       "      <td>228</td>\n",
       "      <td>0.410811</td>\n",
       "      <td>107</td>\n",
       "      <td>0.192793</td>\n",
       "      <td>632</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>2844</td>\n",
       "      <td>596</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.771812</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>24.833333</td>\n",
       "      <td>593.658810</td>\n",
       "      <td>0.996072</td>\n",
       "      <td>279</td>\n",
       "      <td>0.468121</td>\n",
       "      <td>138</td>\n",
       "      <td>0.231544</td>\n",
       "      <td>626</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>1208</td>\n",
       "      <td>242</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.991736</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>18.615385</td>\n",
       "      <td>237.327684</td>\n",
       "      <td>0.980693</td>\n",
       "      <td>135</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>58</td>\n",
       "      <td>0.239669</td>\n",
       "      <td>244</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4039</td>\n",
       "      <td>817</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.943696</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>17.382979</td>\n",
       "      <td>812.656033</td>\n",
       "      <td>0.994683</td>\n",
       "      <td>386</td>\n",
       "      <td>0.472460</td>\n",
       "      <td>210</td>\n",
       "      <td>0.257038</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>2448</td>\n",
       "      <td>468</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.230769</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>21.272727</td>\n",
       "      <td>465.656652</td>\n",
       "      <td>0.994993</td>\n",
       "      <td>224</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>101</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>540</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>1081</td>\n",
       "      <td>214</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.051402</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19.454545</td>\n",
       "      <td>212.990566</td>\n",
       "      <td>0.995283</td>\n",
       "      <td>114</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>63</td>\n",
       "      <td>0.294393</td>\n",
       "      <td>259</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>2094</td>\n",
       "      <td>433</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.836028</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>22.789474</td>\n",
       "      <td>426.651090</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>221</td>\n",
       "      <td>0.510393</td>\n",
       "      <td>121</td>\n",
       "      <td>0.279446</td>\n",
       "      <td>501</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "0       1623   4332    900      28           13             0   \n",
       "1       1143   1465    280      11            3             1   \n",
       "2        660   1696    325      17            2             0   \n",
       "3       1596   2640    555      20           17             0   \n",
       "4        846   2844    596      33            4             1   \n",
       "..       ...    ...    ...     ...          ...           ...   \n",
       "194     1226   1208    242       8            8             0   \n",
       "195      862   4039    817      24           11             1   \n",
       "196     1562   2448    468      22            7             0   \n",
       "197     1336   1081    214      14            5             0   \n",
       "198     1171   2094    433      11           12             0   \n",
       "\n",
       "     avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "0           4.813333         39          1          23.076923  893.988852   \n",
       "1           5.232143         14          3          20.000000  278.321343   \n",
       "2           5.218462         19          1          17.105263  321.316770   \n",
       "3           4.756757         28          0          19.821429  551.989150   \n",
       "4           4.771812         24          9          24.833333  593.658810   \n",
       "..               ...        ...        ...                ...         ...   \n",
       "194         4.991736         13          0          18.615385  237.327684   \n",
       "195         4.943696         47          2          17.382979  812.656033   \n",
       "196         5.230769         22          0          21.272727  465.656652   \n",
       "197         5.051402         11          0          19.454545  212.990566   \n",
       "198         4.836028         19          0          22.789474  426.651090   \n",
       "\n",
       "     POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           0.993321           392                  0.435556            196   \n",
       "1           0.994005           131                  0.467857             51   \n",
       "2           0.988667           178                  0.547692             92   \n",
       "3           0.994575           228                  0.410811            107   \n",
       "4           0.996072           279                  0.468121            138   \n",
       "..               ...           ...                       ...            ...   \n",
       "194         0.980693           135                  0.557851             58   \n",
       "195         0.994683           386                  0.472460            210   \n",
       "196         0.994993           224                  0.478632            101   \n",
       "197         0.995283           114                  0.532710             63   \n",
       "198         0.985337           221                  0.510393            121   \n",
       "\n",
       "     synonym_words/total_words  unstemmed  stemmed  \n",
       "0                     0.217778        750      750  \n",
       "1                     0.182143        339      316  \n",
       "2                     0.283077        352      337  \n",
       "3                     0.192793        632      605  \n",
       "4                     0.231544        626      607  \n",
       "..                         ...        ...      ...  \n",
       "194                   0.239669        244      242  \n",
       "195                   0.257038        750      750  \n",
       "196                   0.215812        540      526  \n",
       "197                   0.294393        259      256  \n",
       "198                   0.279446        501      478  \n",
       "\n",
       "[199 rows x 18 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we selected specific features when training the SVM model we need to do the same here, i.e. we need to extract the values from the same columns that were used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the column headers of the features we want to keep\n",
    "selected_cols = ['unstemmed', 'stemmed', 'chars', 'POS', 'words', 'prompt_words', 'synonym_words', 'commas']\n",
    "# extract the values only from the selected columns\n",
    "selected_features = kaggle_data[selected_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data using the same scaler (MinMaxScaler()) that was used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data with the same scaler used for the training and test set\n",
    "X_kaggle = scaler.transform(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions using the SVM model and display these predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = opt_svm.predict(X_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 3,\n",
       "       3, 4, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 3, 4, 3, 3, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4, 2, 3, 3, 3, 3, 4,\n",
       "       4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4,\n",
       "       2, 3, 3, 3, 4, 3, 4, 4, 4, 4, 4, 3, 4, 5, 2, 3, 3, 4, 3, 3, 4, 4,\n",
       "       3, 3, 4, 4, 4, 3, 3, 4, 2, 3, 4, 3, 4, 3, 3, 4, 3, 4, 3, 2, 4, 4,\n",
       "       2, 3, 4, 3, 3, 4, 1, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 4, 3,\n",
       "       4, 3, 3, 3, 4, 4, 4, 3, 2, 3, 4, 3, 3, 3, 2, 4, 3, 4, 4, 3, 4, 4,\n",
       "       4, 3, 3, 5, 4, 3, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3,\n",
       "       3], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Output to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the results to a csv file, we must first create a DataFrame with the appropriate format:\n",
    "- Columns (2 columns):\n",
    "    - *essayid*: the unique id for that essay\n",
    "    - *score*: the score for that essay\n",
    "- 200 lines:\n",
    "    - 1 header containing the column names\n",
    "    - 199 entries of essay-score pairs\n",
    "    \n",
    "To do this we can extract the essay ids from the *kaggle_data* DataFrame, and append a new column *score* filled with the model's predictions for each essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  score\n",
       "0       1623      4\n",
       "1       1143      3\n",
       "2        660      4\n",
       "3       1596      4\n",
       "4        846      4\n",
       "..       ...    ...\n",
       "194     1226      3\n",
       "195      862      4\n",
       "196     1562      4\n",
       "197     1336      3\n",
       "198     1171      3\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the essayid column from kaggle_data\n",
    "df_predictions = kaggle_data[\"essayid\"]\n",
    "# convert the Series to a DataFrame\n",
    "df_predictions = df_predictions.to_frame()\n",
    "# create new column score and fill it with the model's predictions\n",
    "df_predictions[\"score\"] = pd.Series(kaggle_predictions)\n",
    "\n",
    "# display the DataFrame\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write this resulting DataFrame to a csv file by using the *.to_csv()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to run the following line of code, uncomment the line and change the path to a valid location in your computer\n",
    "# df_predictions.to_csv(r'C:\\Users\\Lenovo\\Documents\\Monash_Programming\\Sem 1 2021\\FIT1043\\Assignment_2\\Predictions_With_SVM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;Using a Random Forest Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the SVM, the random forest classifier performs better when provided with all the features therefore we are not going to do feature selection here except for excluding the *essayid* column which simply provides the unique id of the essay and is not an indication of the essay's contents or quality thus it has no bearing on the score of the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = essay_data.iloc[:, 1:-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing to note about the data is that there is a large imbalance between the frequency of the scores. The majority of essays are scored either a 3 or a 4. We can see this by using the *Counter* class from the *collections* library which counts the number of essays for each score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 583, 3: 557, 2: 110, 1: 18, 5: 60, 6: 4})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there are 583 essays that scored a 4 which makes up **43.8%** of the total essays whereas there are only 4 essays with a score of 6, this amounts to **0.45%** of total essays. This means that we are dealing with an imbalanced classification problem which is a classification problem in which the distribution of examples across the classes is skewed. This results in models having difficulty predicting these minority classes thus resulting in poorer performance overall.\n",
    "\n",
    "To combat this we can oversample these minority classes by using the Syntethic Minority Oversampling Technique which synthesizes new examples for these minority classes from existing ones. For this, we can use the *SMOTE* class from the *imblearn.over_sampling* module and resample the dataset from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_neighbors is the number of nearest neighbours to construct syntethic samples\n",
    "# set random_state to an integer so that output is reproducible\n",
    "oversample = SMOTE(k_neighbors = 1, random_state = 209476)\n",
    "\n",
    "# resample the dataset\n",
    "features_over, label_over = oversample.fit_resample(all_features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the oversampled dataset into the training and test set with the ratio 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size parameter determines the proportion of the test set overt the entire dataset\n",
    "# an integer is passed in random_state to ensure reproducible output\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_over, label_over, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the feature values using the *StandardScaler()* function from the *sklearn.preprocessing* module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scaler as the StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data which computes the mean and standard deviation of every feature in X_train\n",
    "# then we transform X_train to scale the data according to the computed mean and standard deviation\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Here we transform X_test using the mean and standard deviation of the corresponding features in X_train\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the random forest classifier and use GridSearchCV to search for the best parameters for this model based on the training set. Here the most important parameters are:\n",
    "- n_estimators: the number of trees to use\n",
    "- max_depth: the maximum depth of the tree\n",
    "\n",
    "The following block of code should only take ~5-15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5; 1/16] START max_depth=2, n_estimators=5................................\n",
      "[CV 1/5; 1/16] END ..............max_depth=2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5; 1/16] START max_depth=2, n_estimators=5................................\n",
      "[CV 2/5; 1/16] END ..............max_depth=2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5; 1/16] START max_depth=2, n_estimators=5................................\n",
      "[CV 3/5; 1/16] END ..............max_depth=2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5; 1/16] START max_depth=2, n_estimators=5................................\n",
      "[CV 4/5; 1/16] END ..............max_depth=2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5; 1/16] START max_depth=2, n_estimators=5................................\n",
      "[CV 5/5; 1/16] END ..............max_depth=2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5; 2/16] START max_depth=2, n_estimators=10...............................\n",
      "[CV 1/5; 2/16] END .............max_depth=2, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5; 2/16] START max_depth=2, n_estimators=10...............................\n",
      "[CV 2/5; 2/16] END .............max_depth=2, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5; 2/16] START max_depth=2, n_estimators=10...............................\n",
      "[CV 3/5; 2/16] END .............max_depth=2, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5; 2/16] START max_depth=2, n_estimators=10...............................\n",
      "[CV 4/5; 2/16] END .............max_depth=2, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5; 2/16] START max_depth=2, n_estimators=10...............................\n",
      "[CV 5/5; 2/16] END .............max_depth=2, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5; 3/16] START max_depth=2, n_estimators=15...............................\n",
      "[CV 1/5; 3/16] END .............max_depth=2, n_estimators=15; total time=   0.0s\n",
      "[CV 2/5; 3/16] START max_depth=2, n_estimators=15...............................\n",
      "[CV 2/5; 3/16] END .............max_depth=2, n_estimators=15; total time=   0.0s\n",
      "[CV 3/5; 3/16] START max_depth=2, n_estimators=15...............................\n",
      "[CV 3/5; 3/16] END .............max_depth=2, n_estimators=15; total time=   0.0s\n",
      "[CV 4/5; 3/16] START max_depth=2, n_estimators=15...............................\n",
      "[CV 4/5; 3/16] END .............max_depth=2, n_estimators=15; total time=   0.0s\n",
      "[CV 5/5; 3/16] START max_depth=2, n_estimators=15...............................\n",
      "[CV 5/5; 3/16] END .............max_depth=2, n_estimators=15; total time=   0.0s\n",
      "[CV 1/5; 4/16] START max_depth=2, n_estimators=20...............................\n",
      "[CV 1/5; 4/16] END .............max_depth=2, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5; 4/16] START max_depth=2, n_estimators=20...............................\n",
      "[CV 2/5; 4/16] END .............max_depth=2, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5; 4/16] START max_depth=2, n_estimators=20...............................\n",
      "[CV 3/5; 4/16] END .............max_depth=2, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5; 4/16] START max_depth=2, n_estimators=20...............................\n",
      "[CV 4/5; 4/16] END .............max_depth=2, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5; 4/16] START max_depth=2, n_estimators=20...............................\n",
      "[CV 5/5; 4/16] END .............max_depth=2, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5; 5/16] START max_depth=5, n_estimators=5................................\n",
      "[CV 1/5; 5/16] END ..............max_depth=5, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5; 5/16] START max_depth=5, n_estimators=5................................\n",
      "[CV 2/5; 5/16] END ..............max_depth=5, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5; 5/16] START max_depth=5, n_estimators=5................................\n",
      "[CV 3/5; 5/16] END ..............max_depth=5, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5; 5/16] START max_depth=5, n_estimators=5................................\n",
      "[CV 4/5; 5/16] END ..............max_depth=5, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5; 5/16] START max_depth=5, n_estimators=5................................\n",
      "[CV 5/5; 5/16] END ..............max_depth=5, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5; 6/16] START max_depth=5, n_estimators=10...............................\n",
      "[CV 1/5; 6/16] END .............max_depth=5, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5; 6/16] START max_depth=5, n_estimators=10...............................\n",
      "[CV 2/5; 6/16] END .............max_depth=5, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5; 6/16] START max_depth=5, n_estimators=10...............................\n",
      "[CV 3/5; 6/16] END .............max_depth=5, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5; 6/16] START max_depth=5, n_estimators=10...............................\n",
      "[CV 4/5; 6/16] END .............max_depth=5, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5; 6/16] START max_depth=5, n_estimators=10...............................\n",
      "[CV 5/5; 6/16] END .............max_depth=5, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5; 7/16] START max_depth=5, n_estimators=15...............................\n",
      "[CV 1/5; 7/16] END .............max_depth=5, n_estimators=15; total time=   0.0s\n",
      "[CV 2/5; 7/16] START max_depth=5, n_estimators=15...............................\n",
      "[CV 2/5; 7/16] END .............max_depth=5, n_estimators=15; total time=   0.0s\n",
      "[CV 3/5; 7/16] START max_depth=5, n_estimators=15...............................\n",
      "[CV 3/5; 7/16] END .............max_depth=5, n_estimators=15; total time=   0.0s\n",
      "[CV 4/5; 7/16] START max_depth=5, n_estimators=15...............................\n",
      "[CV 4/5; 7/16] END .............max_depth=5, n_estimators=15; total time=   0.0s\n",
      "[CV 5/5; 7/16] START max_depth=5, n_estimators=15...............................\n",
      "[CV 5/5; 7/16] END .............max_depth=5, n_estimators=15; total time=   0.0s\n",
      "[CV 1/5; 8/16] START max_depth=5, n_estimators=20...............................\n",
      "[CV 1/5; 8/16] END .............max_depth=5, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5; 8/16] START max_depth=5, n_estimators=20...............................\n",
      "[CV 2/5; 8/16] END .............max_depth=5, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5; 8/16] START max_depth=5, n_estimators=20...............................\n",
      "[CV 3/5; 8/16] END .............max_depth=5, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5; 8/16] START max_depth=5, n_estimators=20...............................\n",
      "[CV 4/5; 8/16] END .............max_depth=5, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5; 8/16] START max_depth=5, n_estimators=20...............................\n",
      "[CV 5/5; 8/16] END .............max_depth=5, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5; 9/16] START max_depth=7, n_estimators=5................................\n",
      "[CV 1/5; 9/16] END ..............max_depth=7, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5; 9/16] START max_depth=7, n_estimators=5................................\n",
      "[CV 2/5; 9/16] END ..............max_depth=7, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5; 9/16] START max_depth=7, n_estimators=5................................\n",
      "[CV 3/5; 9/16] END ..............max_depth=7, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5; 9/16] START max_depth=7, n_estimators=5................................\n",
      "[CV 4/5; 9/16] END ..............max_depth=7, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5; 9/16] START max_depth=7, n_estimators=5................................\n",
      "[CV 5/5; 9/16] END ..............max_depth=7, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5; 10/16] START max_depth=7, n_estimators=10..............................\n",
      "[CV 1/5; 10/16] END ............max_depth=7, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5; 10/16] START max_depth=7, n_estimators=10..............................\n",
      "[CV 2/5; 10/16] END ............max_depth=7, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5; 10/16] START max_depth=7, n_estimators=10..............................\n",
      "[CV 3/5; 10/16] END ............max_depth=7, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5; 10/16] START max_depth=7, n_estimators=10..............................\n",
      "[CV 4/5; 10/16] END ............max_depth=7, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5; 10/16] START max_depth=7, n_estimators=10..............................\n",
      "[CV 5/5; 10/16] END ............max_depth=7, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5; 11/16] START max_depth=7, n_estimators=15..............................\n",
      "[CV 1/5; 11/16] END ............max_depth=7, n_estimators=15; total time=   0.0s\n",
      "[CV 2/5; 11/16] START max_depth=7, n_estimators=15..............................\n",
      "[CV 2/5; 11/16] END ............max_depth=7, n_estimators=15; total time=   0.0s\n",
      "[CV 3/5; 11/16] START max_depth=7, n_estimators=15..............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 11/16] END ............max_depth=7, n_estimators=15; total time=   0.0s\n",
      "[CV 4/5; 11/16] START max_depth=7, n_estimators=15..............................\n",
      "[CV 4/5; 11/16] END ............max_depth=7, n_estimators=15; total time=   0.0s\n",
      "[CV 5/5; 11/16] START max_depth=7, n_estimators=15..............................\n",
      "[CV 5/5; 11/16] END ............max_depth=7, n_estimators=15; total time=   0.0s\n",
      "[CV 1/5; 12/16] START max_depth=7, n_estimators=20..............................\n",
      "[CV 1/5; 12/16] END ............max_depth=7, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5; 12/16] START max_depth=7, n_estimators=20..............................\n",
      "[CV 2/5; 12/16] END ............max_depth=7, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5; 12/16] START max_depth=7, n_estimators=20..............................\n",
      "[CV 3/5; 12/16] END ............max_depth=7, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5; 12/16] START max_depth=7, n_estimators=20..............................\n",
      "[CV 4/5; 12/16] END ............max_depth=7, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5; 12/16] START max_depth=7, n_estimators=20..............................\n",
      "[CV 5/5; 12/16] END ............max_depth=7, n_estimators=20; total time=   0.1s\n",
      "[CV 1/5; 13/16] START max_depth=9, n_estimators=5...............................\n",
      "[CV 1/5; 13/16] END .............max_depth=9, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5; 13/16] START max_depth=9, n_estimators=5...............................\n",
      "[CV 2/5; 13/16] END .............max_depth=9, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5; 13/16] START max_depth=9, n_estimators=5...............................\n",
      "[CV 3/5; 13/16] END .............max_depth=9, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5; 13/16] START max_depth=9, n_estimators=5...............................\n",
      "[CV 4/5; 13/16] END .............max_depth=9, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5; 13/16] START max_depth=9, n_estimators=5...............................\n",
      "[CV 5/5; 13/16] END .............max_depth=9, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5; 14/16] START max_depth=9, n_estimators=10..............................\n",
      "[CV 1/5; 14/16] END ............max_depth=9, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5; 14/16] START max_depth=9, n_estimators=10..............................\n",
      "[CV 2/5; 14/16] END ............max_depth=9, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5; 14/16] START max_depth=9, n_estimators=10..............................\n",
      "[CV 3/5; 14/16] END ............max_depth=9, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5; 14/16] START max_depth=9, n_estimators=10..............................\n",
      "[CV 4/5; 14/16] END ............max_depth=9, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5; 14/16] START max_depth=9, n_estimators=10..............................\n",
      "[CV 5/5; 14/16] END ............max_depth=9, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5; 15/16] START max_depth=9, n_estimators=15..............................\n",
      "[CV 1/5; 15/16] END ............max_depth=9, n_estimators=15; total time=   0.0s\n",
      "[CV 2/5; 15/16] START max_depth=9, n_estimators=15..............................\n",
      "[CV 2/5; 15/16] END ............max_depth=9, n_estimators=15; total time=   0.0s\n",
      "[CV 3/5; 15/16] START max_depth=9, n_estimators=15..............................\n",
      "[CV 3/5; 15/16] END ............max_depth=9, n_estimators=15; total time=   0.0s\n",
      "[CV 4/5; 15/16] START max_depth=9, n_estimators=15..............................\n",
      "[CV 4/5; 15/16] END ............max_depth=9, n_estimators=15; total time=   0.0s\n",
      "[CV 5/5; 15/16] START max_depth=9, n_estimators=15..............................\n",
      "[CV 5/5; 15/16] END ............max_depth=9, n_estimators=15; total time=   0.0s\n",
      "[CV 1/5; 16/16] START max_depth=9, n_estimators=20..............................\n",
      "[CV 1/5; 16/16] END ............max_depth=9, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5; 16/16] START max_depth=9, n_estimators=20..............................\n",
      "[CV 2/5; 16/16] END ............max_depth=9, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5; 16/16] START max_depth=9, n_estimators=20..............................\n",
      "[CV 3/5; 16/16] END ............max_depth=9, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5; 16/16] START max_depth=9, n_estimators=20..............................\n",
      "[CV 4/5; 16/16] END ............max_depth=9, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5; 16/16] START max_depth=9, n_estimators=20..............................\n",
      "[CV 5/5; 16/16] END ............max_depth=9, n_estimators=20; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={'max_depth': [2, 5, 7, 9],\n",
       "                         'n_estimators': [5, 10, 15, 20]},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the RandomForestClassifier\n",
    "# provide an integer for random_state so that the output is reproducible\n",
    "rfc = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# provide list of possible parameter values\n",
    "parameters={'n_estimators' : [5, 10, 15, 20],\n",
    "            'max_depth' : [2, 5, 7, 9]}\n",
    "\n",
    "# in the first parameter the estimator for which we want to test out the parameter combinations is passed in\n",
    "# In the second parameter, try every combination of parameter values for the provided estimator (rfc)\n",
    "# verbose parameter controls its verbosity, a value of 1 just displays the total number of combinations\n",
    "opt_rfc = GridSearchCV(rfc, parameters, verbose=10)\n",
    "# fit the rfc to the training set and try all the parameter combinations to find the best one\n",
    "opt_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been fitted to the training set and the best parameters have been chosen, we can see which of the parameter values had the best result by accessing the *best_params_* attribute of GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 20}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of the random forest classifier, we must first use it to predict the scores for the essays in the test set by calling the *.predict()* method of the model on *X_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rfc_predictions = opt_rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute and display the confusion matrix to visualize the performance of the model. Note here that totaling the values along a row (which gives the total number of essays with that score) might give a different total than the first confusion matrix, this is the result of the oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x236826cfaf0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the confusion matrix using the actual scores of the essays in the test set and the model predictions\n",
    "cm = confusion_matrix(y_test, opt_rfc_predictions)\n",
    "# first parameter is the confusion matrix computed\n",
    "# display_labels parameter defines the list of possible labels (from 1 to 6), since essay scores range from 1-6\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[1,2,3,4,5,6])\n",
    "\n",
    "# display the confusion matrix\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the diagonal (upper-left to bottom-right) is lit up and other squares *not* on the diagonal are dark, this means that most of the model's predictions are correct and therefore the model seems to perform quite well. We can reinforce this notion by displaying the QWK score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9747343352878785\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_test, opt_rfc_predictions, weights='quadratic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this score is a huge improvement over the score we obtained for our SVM (0.713) so we have successfully created a very accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Read File and Predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to read the *'FIT1043-Essay-Features-Submission.csv'* file and convert it to a DataFrame by using the *.read_csv()* function from the *pandas* library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file, convert it into a DataFrame\n",
    "kaggle_data = pd.read_csv('FIT1043-Essay-Features-Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should confirm that the file has been read properly by displaying the DataFrame's shape and comparing it to the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 18)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we know that the *kaggle_data* DataFrame has 199 rows and 18 columns, and from opening the original csv file we can see that it also has 199 rows (excluding the first row which is used as a header in the DataFrame and is thus not counted as a row) and 18 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can glimpse at the contents of the DataFrame by displaying the first and last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4332</td>\n",
       "      <td>900</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.813333</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>893.988852</td>\n",
       "      <td>0.993321</td>\n",
       "      <td>392</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>196</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>1465</td>\n",
       "      <td>280</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.232143</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>278.321343</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>131</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>51</td>\n",
       "      <td>0.182143</td>\n",
       "      <td>339</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>1696</td>\n",
       "      <td>325</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.218462</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17.105263</td>\n",
       "      <td>321.316770</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>178</td>\n",
       "      <td>0.547692</td>\n",
       "      <td>92</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>352</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>2640</td>\n",
       "      <td>555</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4.756757</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.821429</td>\n",
       "      <td>551.989150</td>\n",
       "      <td>0.994575</td>\n",
       "      <td>228</td>\n",
       "      <td>0.410811</td>\n",
       "      <td>107</td>\n",
       "      <td>0.192793</td>\n",
       "      <td>632</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>2844</td>\n",
       "      <td>596</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.771812</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>24.833333</td>\n",
       "      <td>593.658810</td>\n",
       "      <td>0.996072</td>\n",
       "      <td>279</td>\n",
       "      <td>0.468121</td>\n",
       "      <td>138</td>\n",
       "      <td>0.231544</td>\n",
       "      <td>626</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0     1623   4332    900      28           13             0         4.813333   \n",
       "1     1143   1465    280      11            3             1         5.232143   \n",
       "2      660   1696    325      17            2             0         5.218462   \n",
       "3     1596   2640    555      20           17             0         4.756757   \n",
       "4      846   2844    596      33            4             1         4.771812   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         39          1          23.076923  893.988852         0.993321   \n",
       "1         14          3          20.000000  278.321343         0.994005   \n",
       "2         19          1          17.105263  321.316770         0.988667   \n",
       "3         28          0          19.821429  551.989150         0.994575   \n",
       "4         24          9          24.833333  593.658810         0.996072   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           392                  0.435556            196   \n",
       "1           131                  0.467857             51   \n",
       "2           178                  0.547692             92   \n",
       "3           228                  0.410811            107   \n",
       "4           279                  0.468121            138   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  \n",
       "0                   0.217778        750      750  \n",
       "1                   0.182143        339      316  \n",
       "2                   0.283077        352      337  \n",
       "3                   0.192793        632      605  \n",
       "4                   0.231544        626      607  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>1208</td>\n",
       "      <td>242</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.991736</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>18.615385</td>\n",
       "      <td>237.327684</td>\n",
       "      <td>0.980693</td>\n",
       "      <td>135</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>58</td>\n",
       "      <td>0.239669</td>\n",
       "      <td>244</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4039</td>\n",
       "      <td>817</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.943696</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>17.382979</td>\n",
       "      <td>812.656033</td>\n",
       "      <td>0.994683</td>\n",
       "      <td>386</td>\n",
       "      <td>0.472460</td>\n",
       "      <td>210</td>\n",
       "      <td>0.257038</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>2448</td>\n",
       "      <td>468</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.230769</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>21.272727</td>\n",
       "      <td>465.656652</td>\n",
       "      <td>0.994993</td>\n",
       "      <td>224</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>101</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>540</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>1081</td>\n",
       "      <td>214</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.051402</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19.454545</td>\n",
       "      <td>212.990566</td>\n",
       "      <td>0.995283</td>\n",
       "      <td>114</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>63</td>\n",
       "      <td>0.294393</td>\n",
       "      <td>259</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>2094</td>\n",
       "      <td>433</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.836028</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>22.789474</td>\n",
       "      <td>426.651090</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>221</td>\n",
       "      <td>0.510393</td>\n",
       "      <td>121</td>\n",
       "      <td>0.279446</td>\n",
       "      <td>501</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "194     1226   1208    242       8            8             0   \n",
       "195      862   4039    817      24           11             1   \n",
       "196     1562   2448    468      22            7             0   \n",
       "197     1336   1081    214      14            5             0   \n",
       "198     1171   2094    433      11           12             0   \n",
       "\n",
       "     avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "194         4.991736         13          0          18.615385  237.327684   \n",
       "195         4.943696         47          2          17.382979  812.656033   \n",
       "196         5.230769         22          0          21.272727  465.656652   \n",
       "197         5.051402         11          0          19.454545  212.990566   \n",
       "198         4.836028         19          0          22.789474  426.651090   \n",
       "\n",
       "     POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "194         0.980693           135                  0.557851             58   \n",
       "195         0.994683           386                  0.472460            210   \n",
       "196         0.994993           224                  0.478632            101   \n",
       "197         0.995283           114                  0.532710             63   \n",
       "198         0.985337           221                  0.510393            121   \n",
       "\n",
       "     synonym_words/total_words  unstemmed  stemmed  \n",
       "194                   0.239669        244      242  \n",
       "195                   0.257038        750      750  \n",
       "196                   0.215812        540      526  \n",
       "197                   0.294393        259      256  \n",
       "198                   0.279446        501      478  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most notable thing here is that it differs from the original dataset as it is missing the score column, this is because we are meant to fill this column with the predictions of our trained model. To do this, we must first isolate the useful features and transform it with the scaler that was used for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the feature values to be used\n",
    "kaggle_features = kaggle_data.iloc[:, 1:].values\n",
    "# scale the data with the same scaler used for the training and test set\n",
    "X_kaggle = scaler.transform(kaggle_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the features have been properly scaled, we can use the *.predict()* function of the model to obtain the predictions of our trained model for these unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = opt_rfc.predict(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the predictions by displaying the *kaggle_predictions* array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 3,\n",
       "       4, 4, 4, 4, 4, 4, 4, 3, 2, 4, 3, 3, 4, 3, 5, 4, 3, 3, 3, 4, 3, 3,\n",
       "       1, 3, 3, 4, 4, 3, 2, 4, 4, 4, 3, 4, 3, 4, 4, 4, 2, 3, 3, 4, 3, 4,\n",
       "       4, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 5, 4, 4, 4, 4, 3, 4, 2, 4, 4,\n",
       "       2, 3, 3, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 5, 2, 3, 3, 4, 3, 3, 5, 4,\n",
       "       4, 3, 4, 4, 5, 3, 2, 4, 2, 3, 4, 4, 5, 3, 2, 4, 3, 5, 4, 2, 4, 4,\n",
       "       2, 3, 4, 4, 3, 4, 1, 4, 4, 4, 4, 3, 3, 4, 3, 3, 5, 4, 4, 4, 4, 3,\n",
       "       4, 3, 3, 3, 4, 4, 4, 3, 2, 2, 4, 5, 2, 3, 2, 3, 3, 4, 3, 3, 4, 4,\n",
       "       4, 3, 3, 4, 4, 2, 4, 4, 4, 4, 3, 4, 3, 3, 3, 5, 3, 4, 3, 5, 4, 3,\n",
       "       3], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that only valid scores have been predicted by displaying the maximum and minimum scores predicted and ensuring that they are within the range 1 to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum score predicted: 1\n",
      "Maximum score predicted: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum score predicted:\", kaggle_predictions.min())\n",
    "print(\"Maximum score predicted:\", kaggle_predictions.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;Output to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the results to a csv file, we must first create a DataFrame with the appropriate format:\n",
    "- Columns (2 columns):\n",
    "    - *essayid*: the unique id for that essay\n",
    "    - *score*: the score for that essay\n",
    "- 200 lines:\n",
    "    - 1 header containing the column names\n",
    "    - 199 entries of essay-score pairs\n",
    "    \n",
    "To do this we can extract the essay ids from the *kaggle_data* DataFrame, and append a new column *score* filled with the model's predictions for each essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  score\n",
       "0       1623      5\n",
       "1       1143      3\n",
       "2        660      3\n",
       "3       1596      4\n",
       "4        846      4\n",
       "..       ...    ...\n",
       "194     1226      3\n",
       "195      862      5\n",
       "196     1562      4\n",
       "197     1336      3\n",
       "198     1171      3\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the essayid column from kaggle_data\n",
    "df_predictions = kaggle_data[\"essayid\"]\n",
    "# convert the Series to a DataFrame\n",
    "df_predictions = df_predictions.to_frame()\n",
    "# create new column score and fill it with the model's predictions\n",
    "df_predictions[\"score\"] = pd.Series(kaggle_predictions)\n",
    "\n",
    "# display the DataFrame\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write this resulting DataFrame to a csv file by using the *.to_csv()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to run the following line of code, uncomment the line and change the path to a valid location in your computer\n",
    "# df_predictions.to_csv(r'C:\\Users\\Lenovo\\Documents\\Monash_Programming\\Sem 1 2021\\FIT1043\\Assignment_2\\31989101-MichelleAdeline-31.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report I have read, and analyzed the provided dataset of essay information and I recognized that this problem of automated essay scoring is a multi-class classification problem. Then, I used two supervised machine learning algorithms - Support Vector Machine (SVM) and Random Forest Classifier in order to predict the essay scores. \n",
    "\n",
    "Before feeding the data to both algorithms it was first processed in order to improve performance. This involves feature selection, oversampling, and standardizing/normalizing after splitting the data. When training the models on the training set, I used hyperparameter tuning in order to select the best parameters to obtain the best performing model. Afterwards both models are evaluated by using a confusion matrix and the Quadratic Weighted Kappa metric for which the SVM model received a score of 0.713 for its predictions on the test set and the Random Forest Classifier received a score of 0.72775 from kaggle for its predictions on the unlabeled data provided. \n",
    "\n",
    "With this I have successfully developed two different algorithms capable of automated essay scoring, I learned about and used the Support Vector Machine algorithm to build a model, I understood the importance of preprocessing, oversampling, and choosing the best parameters for a model in order to improve its performance, as well as how to evaluate its accuracy, and I have gained experience in exploring and learning on my own as well as participating in kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amazon Web Services. (n.d.). What is data labeling for machine learning? Retrieved from https://aws.amazon.com/sagemaker/groundtruth/what-is-data-labeling/\n",
    "- Arora, A. (2019). Quadratic Kappa Metric explained in 5 simple steps. Retrieved from https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps\n",
    "- Band, A. (2020). Multi-class Classification  One-vs-All & One-vs-One. Retrieved from https://towardsdatascience.com/multi-class-classification-one-vs-all-one-vs-one-94daed32a87b\n",
    "- Brownlee, J. (2020). Supervised and Unsupervised Machine Learning Algorithms. Retrieved from https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/\n",
    "- DataFlair. (n.d.). Kernel Functions-Introduction to SVM Kernel & Examples. Retrieved from https://data-flair.training/blogs/svm-kernel-functions/#:~:text=SVM%20algorithms%20use%20a%20set,it%20into%20the%20required%20form.&text=These%20functions%20can%20be%20different,(RBF)%2C%20and%20sigmoid.\n",
    "- Gandhi, R. (2018). Support Vector Machine  Introduction to Machine Learning Algorithms. Retrieved from https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "- Google Developers. (2020). Training and Test Sets: Splitting Data. Retrieved from https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data\n",
    "- Hale, J. (2019). Scale, Standardize, or Normalize with Scikit-Learn. Retrieved from https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "- IBM Cloud Education. (2020). Supervised Learning. Retrieved from https://www.ibm.com/cloud/learn/supervised-learning\n",
    "- Jaitley, U. (2018). Why Data Normalization is necessary for Machine Learning models. Retrieved from https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=Normalization%20is%20a%20technique%20often,dataset%20does%20not%20require%20normalization.\n",
    "- Sharp, T. (2020). An Introduction to Support Vector Regression (SVR). Retrieved from https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2\n",
    "- Suriya. (n.d.). Support Vector Machines  Kernel Explained. Retrieved from https://codingmachinelearning.wordpress.com/2016/07/25/support-vector-machines-kernel-explained/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
